# -*- coding: utf-8 -*-
"""Text_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jb2PEpQaEKAasqcb5nmTQTeFm03luAl
"""
import streamlit as st
import pandas as pd 
import spacy
from gensim.summarization import summarize
from sklearn.decomposition import LatentDirichletAllocation
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Load pre-trained spaCy model
nlp = spacy.load('en_core_web_sm')

# Function for topic modeling
def extract_topics(doc):
    # Vectorize text
    vec = nlp(doc).vector  
    # LDA Model
    lda = LatentDirichletAllocation(n_components=10, random_state=42)  
    lda.fit(vec)
    return [(topic_id, topic) for topic_id, topic in enumerate(lda.components_)]

# Function for NER  
def extract_entities(doc):
    doc = nlp(doc)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# Main function
def main():

  st.title("Advanced Text Analysis App")
  
  # Option to upload CSV
  uploaded_file = st.file_uploader("Upload CSV", type=['csv']) 
  
  if uploaded_file:
    df = pd.read_csv(uploaded_file)

    # Select text column
    text_col =  st.selectbox("Select text column", df.columns)

    # Number of rows to display
    n = st.slider("Number of rows to display", 50, len(df), 100)

    # Extract text
    data = df[text_col].dropna().iloc[:n].values

    # Summarize text
    summary = summarize(str(data))
    st.subheader("Summary")
    st.write(summary)

    # Topic modeling
    topics = extract_topics(str(data))
    st.subheader("Topics")
    for topic_id, topic in topics:
        st.write(f"{topic_id}: {topic}")

    # NER on sample text
    entities = extract_entities(data[0])
    st.subheader("Named Entities")
    st.write(entities)

    # Word Cloud
    wc = WordCloud().generate(summary)
    fig, ax = plt.subplots()
    ax.imshow(wc)
    st.pyplot(fig)
    
if __name__ == '__main__':
    main()
